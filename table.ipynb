{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import imgkit\n",
    "import pandas as pd\n",
    "import re\n",
    "import cohere\n",
    "import random\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cohere.ClientV2(\"R8fQH9pZzw70Ixq3eOmcLKiCaZVS0wHs7eUc82dU\", base_url=\"https://stg.api.cohere.ai\")\n",
    "\n",
    "PROMPT =  \"\"\"Original Text: \n",
    "{raw_text}\\n\n",
    "\n",
    "Translation: \n",
    "{translation}\\n\n",
    "\n",
    "Instruction:\n",
    "Given the original text and its translation, improve the quality of the translation by rephrasing it. \n",
    "Ensure the rephrased translation closely aligns with the original text in meaning, structure, tone, and style. \n",
    "Make the rephrased translation sound natural and fluent in the target language while preserving the core message, correcting any grammatical errors, and retaining all stylistic elements (e.g., enumeration, punctuation, capitalization, spacing, line breaks, etc.) from the original.\n",
    "\n",
    "The output must strictly follow this format:\n",
    "Rephrased Translation: <rephrased translation placeholder>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('table_data/finqa_train.json') as f:\n",
    "    finqa = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = finqa[10]['table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['in millions',\n",
       "  'defined benefit pension plans',\n",
       "  'other postretirement benefit plans gross payments',\n",
       "  'medicare subsidy receipts',\n",
       "  'postemployment benefit plans'],\n",
       " ['2009', '$ 176.3', '$ 56.0', '$ -6.1 ( 6.1 )', '$ 16.6'],\n",
       " ['2010', '182.5', '59.9', '-6.7 ( 6.7 )', '17.5'],\n",
       " ['2011', '189.8', '63.3', '-7.3 ( 7.3 )', '18.1'],\n",
       " ['2012', '197.5', '67.0', '-8.0 ( 8.0 )', '18.8'],\n",
       " ['2013', '206.6', '71.7', '-8.7 ( 8.7 )', '19.4'],\n",
       " ['2014 2013 2018', '1187.3', '406.8', '-55.3 ( 55.3 )', '106.3']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate_v3\n",
    "import time\n",
    "\n",
    "PROJECT_ID = \"valued-sight-253418\"\n",
    "\n",
    "def translate_text(text, target_language_code):\n",
    "    \"\"\"Translating Text.\"\"\"\n",
    "\n",
    "    client = translate_v3.TranslationServiceClient()\n",
    "\n",
    "    parent = f\"projects/{PROJECT_ID}\"\n",
    "\n",
    "    tries = 0\n",
    "\n",
    "    while tries < 10:\n",
    "        try:\n",
    "            response = client.translate_text(\n",
    "                    parent = parent,\n",
    "                    contents= [text],\n",
    "                    mime_type= \"text/plain\",  \n",
    "                    source_language_code= \"en-US\",\n",
    "                    target_language_code= target_language_code,\n",
    "            )\n",
    "            return  response.translations[0].translated_text #[translation.translated_text for translation in response.translations]\n",
    "        \n",
    "        except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(5)\n",
    "                tries += 1\n",
    "                continue\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_table = []\n",
    "for row in data:\n",
    "    translate_row = []\n",
    "    for cell in row:   \n",
    "        if not bool(re.fullmatch(r'^[\\d\\W_]*$', cell)):\n",
    "            translate_cell = translate_text(cell, 'zh-CN')\n",
    "            translate_row.append(translate_cell)\n",
    "        else:\n",
    "            translate_row.append(cell)\n",
    "    translate_table.append(translate_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Styler' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m matplotlib\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mggplot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mstyled_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m()\n\u001b[1;32m     32\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m123.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Adjust dpi as needed\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# # # Convert styled DataFrame to HTML\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# html = styled_df.to_html()\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# # Convert HTML to image\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# imgkit.from_string(html, 'en.jpeg') \u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Styler' object has no attribute 'plot'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(data)\n",
    "header_colors = ['lightgreen', 'green', 'lightsteelblue', 'powderblue', 'sandybrown', 'lightsalmon', 'lightskyblue', 'lightgray']\n",
    "background_colors = ['lightblue', 'aqua', 'cyan', 'honeydew', 'ivory', 'lemonchiffon', 'ghostwhite', 'gainsboro', 'mistyrose', 'powderblue', 'snow', 'whitesmoke', 'lime', 'lightskyblue', ]  \n",
    "\n",
    "styled_df = (\n",
    "    df.style\n",
    "    .hide(axis=\"index\") \n",
    "    .hide(axis=\"columns\") \n",
    "    .set_table_styles([\n",
    "        {'selector': 'tbody', 'props': [('background-color', random.choice(background_colors))]},  \n",
    "        {'selector': 'tbody tr:nth-child(1)', 'props': [('background-color', random.choice(header_colors))]},\n",
    "        {'selector': 'table', 'props': [('border', '1px solid white')]},  \n",
    "        {'selector': 'td', 'props': [('min-width', '150px'), ('max-width', '450px')]}  \n",
    "    ])\n",
    "    .set_properties(**{\n",
    "        'text-align': 'center',  # Center align text\n",
    "        'font-size': '12px',  # Set font size\n",
    "        'padding': '15px',  # Set padding\n",
    "    })\n",
    ")\n",
    "import dataframe_image as dfi\n",
    "\n",
    "# dfi.export(styled_df,f\"xx.jpeg\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "plt.figure()\n",
    "\n",
    "styled_df.plot()\n",
    "\n",
    "plt.savefig('123.jpeg')  # Adjust dpi as needed\n",
    "\n",
    "# # # Convert styled DataFrame to HTML\n",
    "# html = styled_df.to_html()\n",
    "\n",
    "\n",
    "# # Convert HTML to image\n",
    "# imgkit.from_string(html, 'en.jpeg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set\n",
    "import requests\n",
    "from sentence_splitter import split_text_into_sentences\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "def inference_request(url: str, source_language: str, target_language: str, texts: List[str]) -> List[str]:\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"source_language\": source_language,\n",
    "        \"target_language\": target_language,\n",
    "        \"texts\": texts,\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()[\"translated_texts\"]\n",
    "\n",
    "\n",
    "def call_inference_api(\n",
    "    example: Dict[str, List[str]],\n",
    "    url: str,\n",
    "    source_language_code: str,\n",
    "    target_language_code: str,\n",
    "    keys_to_be_translated: List[str],\n",
    ") -> Dict[str, List[str]]:\n",
    "\n",
    "    for key in keys_to_be_translated:\n",
    "        # NLLB model seems to ignore some sentences right before newline characters\n",
    "        batch_str = [sen.replace('\\n', '') for sen in example[key]]\n",
    "        example[key] = inference_request(url, source_language_code, target_language_code, batch_str)\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "def translate_sent_by_sent(\n",
    "    inputs\n",
    ") -> Dict[str, List[str]]:\n",
    "\n",
    "    raw_text, url, source_language_code, target_language_code = inputs\n",
    "    translation = {'text': raw_text}\n",
    "\n",
    "    keys_to_be_translated = [\"text\"]\n",
    "\n",
    "    sentenized_example = defaultdict(list)\n",
    "\n",
    "    for k in keys_to_be_translated:\n",
    "        sentenized_example[f\"{k}_pos\"].append(0)\n",
    "\n",
    "    for k in translation.keys():\n",
    "        sentences = split_text_into_sentences(text=translation[k], language='en')\n",
    "        sentenized_example[k].extend(sentences)\n",
    "        sentenized_example[f\"{k}_pos\"].append(sentenized_example[f\"{k}_pos\"][-1] + len(sentences))\n",
    "    \n",
    "    result = call_inference_api(example=sentenized_example,\n",
    "                                url=url,\n",
    "                                keys_to_be_translated=keys_to_be_translated,\n",
    "                                source_language_code=source_language_code,\n",
    "                                target_language_code=target_language_code)\n",
    "    \n",
    "    for k in keys_to_be_translated:\n",
    "        merged_texts = []\n",
    "        l = 0\n",
    "        r = 1\n",
    "        while r < len(result[f\"{k}_pos\"]):\n",
    "            start = result[f\"{k}_pos\"][l]\n",
    "            end = result[f\"{k}_pos\"][r]\n",
    "            merged_texts.append(' '.join(result[k][start:end]))\n",
    "            l += 1\n",
    "            r += 1\n",
    "        translation[k] = merged_texts[0]\n",
    "    \n",
    "    rephrase_input = PROMPT.format(raw_text=raw_text, translation=translation['text'])\n",
    "\n",
    "    retry_count = 0\n",
    "    response_user = None\n",
    "    response_chatbot = None\n",
    "    while retry_count < 30:\n",
    "        try:\n",
    "            response_user = client.chat(\n",
    "                model='command-r-plus',\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": rephrase_input\n",
    "                    }\n",
    "                ],\n",
    "                temperature = 0.5,\n",
    "                p = 0.9,\n",
    "                max_tokens = 1024,\n",
    "            )\n",
    "            rephrase_output = response_user.message.content[0].text.strip()\n",
    "            match_rephrase_output = re.search(r'Rephrased Translation[:：∶﹕]([\\s\\S]*)', rephrase_output)\n",
    "            if match_rephrase_output:\n",
    "                rephrase_output_extract = match_rephrase_output.group(1).strip()\n",
    "            else:\n",
    "                raise Exception(\"No match found\")\n",
    "            \n",
    "            return rephrase_output_extract\n",
    "        \n",
    "        except Exception as e:\n",
    "            # print(f\"API Error: {e}\")\n",
    "            # print(f\"Retry count: {retry_count}\")\n",
    "            # print(\"Retrying in 10 seconds\")\n",
    "            logging.error(f\"API Error: {e}\")\n",
    "            logging.error(f\"Retry count: {retry_count}\")\n",
    "            logging.error(\"Retrying in 3 seconds\")\n",
    "            if retry_count == 28:\n",
    "                logging.error(f\"Failed: {response_user}\")\n",
    "                logging.error(f\"Failed: {response_chatbot}\")\n",
    "            time.sleep(3)\n",
    "            retry_count += 1\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def translate_dataset_via_inference_api(\n",
    "    dataset,\n",
    "    target_language_code: str,\n",
    "    source_language_code: str,\n",
    "    url: str = \"http://localhost:8000/translate\",\n",
    "    output_dir: str = \"./datasets\",\n",
    ") -> None:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # translated_dataset = []\n",
    "    \n",
    "    # for data in tqdm(dataset):\n",
    "    #     translated_dataset.append(translate_sent_by_sent((data, url, source_language_code, target_language_code)))\n",
    "\n",
    "    translate_table = []\n",
    "    for row in data:\n",
    "        translate_row = []\n",
    "        for cell in row:   \n",
    "            if not bool(re.fullmatch(r'^[\\d\\W_]*$', cell)):\n",
    "                # translate_cell = translate_text(cell, 'zh-CN')\n",
    "                translate_cell = translate_sent_by_sent((cell, url, source_language_code, target_language_code))\n",
    "                translate_row.append(translate_cell)\n",
    "            else:\n",
    "                translate_row.append(cell)\n",
    "        translate_table.append(translate_row)\n",
    "    \n",
    "                           \n",
    "    # print(f\"Translated {len(translated_dataset)} samples\")\n",
    "\n",
    "    # with open(output_dir, \"w\") as f:\n",
    "    #     for data in translated_dataset:\n",
    "    #         f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n",
    "    return translate_table\n",
    "\n",
    "# def translate_dataset(dataset_path,\n",
    "#                       source_language_code: str,\n",
    "#                       target_language_code: str,\n",
    "#                       url: str = \"http://localhost:8000/translate\",\n",
    "#                       output_dir: str = \"./datasets\",) -> None:\n",
    "\n",
    "#     # with open(dataset_path, \"r\") as file:\n",
    "#     #     dataset = [json.loads(line) for line in file]\n",
    "#     # # print(dataset[0])\n",
    "\n",
    "#     # print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "\n",
    "\n",
    "#     translate_dataset_via_inference_api(\n",
    "#         dataset=dataset,\n",
    "#         source_language_code=source_language_code,\n",
    "#         target_language_code=target_language_code,\n",
    "#         url=url,\n",
    "#         output_dir=output_dir,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 9.6726 seconds\n"
     ]
    }
   ],
   "source": [
    "table = translate_dataset_via_inference_api(dataset = None, source_language_code='eng_Latn', target_language_code = 'zho_Hans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_bytes(image):\n",
    "#     img_byte_arr = io.BytesIO()\n",
    "#     image.convert(\"RGB\").save(img_byte_arr, format=\"jpeg\")\n",
    "#     return img_byte_arr.getvalue()\n",
    "import dataframe_image as dfi\n",
    "def covert_to_table_image(table, name):\n",
    "\n",
    "    header_colors = ['lightgreen', 'green', 'lightsteelblue', 'powderblue', 'sandybrown', 'lightsalmon', 'lightskyblue', 'lightgray', 'greenyellow', 'lightseagreen', 'lightslategray','forestgreen', 'mediumspringgreen', 'steelblue', 'mediumpurple' ]\n",
    "    background_colors = ['lightblue', 'aqua', 'cyan', 'honeydew', 'ivory', 'lemonchiffon', 'ghostwhite', 'gainsboro', 'mistyrose', 'powderblue', 'snow', 'whitesmoke', 'lime', 'lightskyblue','khaki', 'mediumaquamarine', 'lightcyan', 'transparent', 'wheat']  \n",
    "\n",
    "    df = pd.DataFrame(table)\n",
    "\n",
    "    styled_df = (\n",
    "        df.style\n",
    "        .hide(axis=\"index\")\n",
    "        .hide(axis=\"columns\")\n",
    "        .set_table_styles([\n",
    "            {'selector': 'tbody tr:nth-child(n+2)', 'props': [('background-color', random.choice(background_colors))]},\n",
    "            {'selector': 'tbody tr:nth-child(1)', 'props': [('background-color', random.choice(header_colors))]},\n",
    "            {'selector': 'table', 'props': [\n",
    "                ('border', '1px solid white'),\n",
    "            ]},\n",
    "            {'selector': 'td', 'props': [\n",
    "                ('min-width', '150px'), \n",
    "                ('max-width', '450px'),\n",
    "                ('padding', '15px'),\n",
    "            ]}\n",
    "        ])\n",
    "        .set_properties(**{\n",
    "            'text-align': 'center',\n",
    "            'font-size': '12px',\n",
    "        })\n",
    "    )\n",
    "\n",
    "    dfi.export(styled_df,f\"images/{name}.jpeg\")\n",
    "\n",
    "    with open(f'images/{name}.jpeg', \"rb\") as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    return f\"data:image/jpeg;base64,{encoded_image}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_expression(expression):\n",
    "#     operations = {\n",
    "#         'add': '+',\n",
    "#         'subtract': '-',\n",
    "#         'multiply': '*',\n",
    "#         'divide': '/',\n",
    "#         'exp': '**',\n",
    "#         'greater': '>'\n",
    "#     }\n",
    "    \n",
    "#     if not isinstance(expression, str):\n",
    "#         return str(expression)\n",
    "    \n",
    "#     # Recursive case: parse the expression\n",
    "#     for operation, symbol in operations.items():\n",
    "#         if expression.startswith(f\"{operation}(\"):\n",
    "#             # Find the content within the parentheses\n",
    "#             inner_expr = expression[len(operation) + 1 : -1]  # Remove the operation and outer parentheses\n",
    "#             # Split the inner expression by the first comma to separate arguments\n",
    "#             arg1, arg2 = split_args(inner_expr)\n",
    "#             # Recursively convert arguments\n",
    "#             arg1_str = convert_expression(arg1.strip())\n",
    "#             arg2_str = convert_expression(arg2.strip())\n",
    "#             # Return the formatted string\n",
    "#             return f\"({arg1_str} {symbol} {arg2_str})\"\n",
    "    \n",
    "#     return expression\n",
    "\n",
    "# def split_args(inner_expr):\n",
    "#     \"\"\"Splits the arguments in the form 'arg1, arg2' handling nested expressions.\"\"\"\n",
    "#     open_parens = 0\n",
    "#     for i, char in enumerate(inner_expr):\n",
    "#         if char == '(':\n",
    "#             open_parens += 1\n",
    "#         elif char == ')':\n",
    "#             open_parens -= 1\n",
    "#         elif char == ',' and open_parens == 0:\n",
    "#             return inner_expr[:i], inner_expr[i + 1:]\n",
    "#     return inner_expr, ''\n",
    "\n",
    "\n",
    "def convert_expression(program):\n",
    "    operation_map = {\n",
    "        'add': '+',\n",
    "        'subtract': '-',\n",
    "        'multiply': '*',\n",
    "        'divide': '/',\n",
    "        'exp': '**',\n",
    "        'greater': '>',\n",
    "        'table_average': 'table_average',\n",
    "        'table_sum': 'table_sum',\n",
    "        'table_max': 'table_max',\n",
    "        'table_min': 'table_min',\n",
    "    }\n",
    "    operation_names = \"|\".join(map(re.escape, operation_map.keys()))\n",
    "    pattern = rf\"({operation_names})\\((.*?)\\)\"\n",
    "\n",
    "    split_operations = re.findall(pattern, program)\n",
    "    \n",
    "    # print(split_operations)\n",
    "    for i, operation in enumerate(split_operations):\n",
    "        op = operation[0]\n",
    "        if op in ['table_average', 'table_sum', 'table_max', 'table_min']:\n",
    "            split_operations[i] = f\"({operation[0]}({operation[1]})\"\n",
    "        else:\n",
    "            op_sym = operation_map[op]\n",
    "            args = operation[1].split(',') #re.findall(r'\\(([^()]+)\\)', operation)[0].split(',')\n",
    "            assert len(args) == 2\n",
    "            arg1 = args[0].strip()\n",
    "            arg2 = args[1].strip()\n",
    "            if \"#\" in arg1:\n",
    "                index = int(arg1.replace(\"#\", \"\"))\n",
    "                arg1 = split_operations[index]\n",
    "            if \"#\" in arg2:\n",
    "                index = int(arg2.replace(\"#\", \"\"))\n",
    "                arg2 = split_operations[index]\n",
    "\n",
    "            split_operations[i] = f\"({arg1} {op_sym} {arg2})\"\n",
    "\n",
    "    return split_operations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 279/6251 [06:14<2:07:37,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 279\n",
      "yes , 38.2%\n",
      "0.38166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 617/6251 [14:09<2:13:07,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 617\n",
      "8.5%\\\\n\n",
      "0.085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 1309/6251 [30:34<2:03:57,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 1309\n",
      "15.7%\\\\n\n",
      "0.15707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1822/6251 [43:06<1:38:40,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 1822\n",
      "1.91% and 3.46% , respectively\n",
      "0.01914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 2096/6251 [50:10<1:54:17,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 2096\n",
      "increased 38.6%\n",
      "0.38633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 2272/6251 [54:45<1:30:35,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 2272\n",
      "\\\\n0.5%\n",
      "0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 2418/6251 [58:21<1:56:10,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 2418\n",
      "the investor made 93.1% more money in the first 5 years compared to the 2016 to 2017 period .\n",
      "192.64126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2641/6251 [1:03:54<1:47:22,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 2641\n",
      "33.9% decrease\n",
      "0.3393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3149/6251 [1:16:38<1:22:27,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 3149\n",
      "the dividend yield increased 0.04% from 2010 to 2012\n",
      "0.00042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 3263/6251 [1:19:44<1:29:27,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 3263\n",
      "yes , 6%\n",
      "162.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 3295/6251 [1:20:33<1:06:27,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 3295\n",
      "56.4%\\\\n\n",
      "0.56359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 3582/6251 [1:27:39<1:00:45,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 3582\n",
      "global payments would have earned an 80.13% greater return than the overall information technology sector .\n",
      "80.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 3639/6251 [1:29:04<1:01:30,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 3639\n",
      "265% increase\n",
      "2.64706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 3794/6251 [1:32:53<54:52,  1.34s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 3794\n",
      "the investor made 93.1% more money in the first 5 years compared to the 2016 to 2017 period .\n",
      "192.64126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 3808/6251 [1:33:15<57:33,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 3808\n",
      "global payments would have earned an 80.13% greater return than the overall information technology sector .\n",
      "80.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 3814/6251 [1:33:23<57:12,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 3814\n",
      "25.8%\\\\n\n",
      "0.25848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 3949/6251 [1:36:52<53:25,  1.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 3949\n",
      "the consolidated net sales grew 211% from 2006 to 2008?\n",
      "2.11269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 4099/6251 [1:40:23<48:27,  1.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 4099\n",
      "6.7%\\\\n\n",
      "0.06685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 4144/6251 [1:41:27<50:38,  1.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 4144\n",
      "\\\\n38.5%\n",
      "0.38477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 4785/6251 [1:57:36<36:25,  1.49s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 4785\n",
      "increased 38.6%\n",
      "0.38633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 4802/6251 [1:58:02<32:38,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 4802\n",
      "17.7% of the segmented sales are converted into operating earnings .\n",
      "0.17741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 4820/6251 [1:58:28<34:34,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 4820\n",
      "18.2% increase\n",
      "43.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 5220/6251 [2:08:22<23:38,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 5220\n",
      "10.8%\\\\n\n",
      "0.10787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 5584/6251 [2:17:28<14:59,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 5584\n",
      "205.07% increase\n",
      "205.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 5696/6251 [2:20:23<12:57,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 5696\n",
      "advance auto parts had a 62.63% greater return than the overall market\n",
      "62.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 5769/6251 [2:22:16<18:47,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 5769\n",
      "$ 35411 or 9.8% increase\n",
      "0.09841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 5814/6251 [2:23:18<09:50,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 5814\n",
      "35% 1316\\\\n\n",
      "0.34726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 5829/6251 [2:23:43<10:32,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 5829\n",
      "8.45%\\\\n\n",
      "0.08454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 5893/6251 [2:25:24<08:15,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 5893\n",
      "6.7%\\\\n\n",
      "0.06715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 6026/6251 [2:28:52<05:34,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 6026\n",
      "5% increase in inventories\n",
      "0.04996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 6077/6251 [2:30:07<03:53,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 6077\n",
      "( 6.13% )\n",
      "6.12805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 6204/6251 [2:33:22<01:08,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 6204\n",
      "the non-us employees had a change of 94.4% greater than the us employees\n",
      "0.9447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 6239/6251 [2:34:17<00:16,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 6239\n",
      "17.1% increase\n",
      "2556557.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 6245/6251 [2:34:25<00:08,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in 6245\n",
      "9.6%\\\\n\n",
      "0.09615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6251/6251 [2:34:35<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "with open('table_data/finqa_train.json') as f:\n",
    "    finqa = json.load(f)\n",
    "\n",
    "with open('table_data/finqa_processed.json' , 'w') as f:\n",
    "    for i, data in enumerate(tqdm(finqa)):\n",
    "\n",
    "        sample = {'Table':[], 'User': [], 'Chatbot': [], 'Image': []}\n",
    "        \n",
    "        sample['Table'].append(data['table'])\n",
    "\n",
    "        User = \"\"\n",
    "        for text in data['pre_text']:\n",
    "            if text != \".\":\n",
    "                User += text.capitalize() + \"\\n\"\n",
    "\n",
    "        for text in data['post_text']:\n",
    "            if text != \".\":\n",
    "                User += text.capitalize() + \"\\n\"\n",
    "\n",
    "        User += data['qa']['question'].capitalize()\n",
    "\n",
    "        sample['User'].append({'text': User, \"language\": \"eng_Latn\", \"source\": \"raw\"})\n",
    "        sample['User'].append({'text': User, \"language\": \"eng_Latn\", \"source\": \"raw-processed\"})\n",
    "\n",
    "        Chatbot = \"\" \n",
    "        if data['qa']['explanation'] != \"\":\n",
    "            Chatbot += \"Rationale:\" + data['qa']['explanation'].capitalize() + \"\\n\"\n",
    "        \n",
    "        # computation = convert_expression(data['qa']['program_re'])\n",
    "        \n",
    "        computation = convert_expression(data['qa']['program'])\n",
    "\n",
    "        if \"const\" in computation:\n",
    "            computation = computation.replace(\"const_\", \"\")\n",
    "            if \"m1\" in computation:\n",
    "                computation = computation.replace(\"m1\", \"-1\")\n",
    "            \n",
    "        Chatbot += \"Computations: \" +  computation  + \"\\n\"\n",
    "        \n",
    "        answer = data['qa']['exe_ans']\n",
    "        try:\n",
    "            if data['qa']['answer'] != \"\":\n",
    "                if ('%' in data['qa']['answer']) and (type(answer) == float):\n",
    "                    if round(float(data['qa']['answer'].replace('%', ''))/100,2) == round(answer, 2):\n",
    "                        answer = data['qa']['answer']\n",
    "        except:\n",
    "            print(f\"Error in {i}\")\n",
    "            print(data['qa']['answer'])\n",
    "            print(answer)\n",
    "            pass\n",
    "\n",
    "        Chatbot += \"Answer: \" + str(answer)\n",
    "\n",
    "        sample['Chatbot'].append({'text': Chatbot, \"language\": \"eng_Latn\", \"source\": \"raw\"})\n",
    "\n",
    "        sample['Image'].append(covert_to_table_image(data['table'], i))\n",
    "\n",
    "        f.write(json.dumps(sample, ensure_ascii=False) + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(0.20886)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('table_data/MultiHiertt_processed.jsonl', 'r') as f, open('table_data/MultiHiertt_processed_2.jsonl', 'w') as output_file:\n",
    "    for i, line in enumerate(f):\n",
    "        data = json.loads(line)\n",
    "        # data['command_id'] = f\"MultiHiertt-{i}\"\n",
    "        # data['metadata'] = {\"source\": \"MultiHiertt\"}\n",
    "        # data['index'] = i\n",
    "        \n",
    "        output_file.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('table_data/MultiHiertt_processed.jsonl', 'r') as f, open('table_data/MultiHiertt_processed_2.jsonl', 'w') as output_file:\n",
    "    for i, line in enumerate(f):\n",
    "        data = json.loads(line)\n",
    "        table_list= []\n",
    "        tables = data['Table'][0]\n",
    "        for table in tables:\n",
    "            table_list.append(pd.read_html(table)[0].fillna(\"\").values.tolist())\n",
    "        data['Table'] = table_list\n",
    "        output_file.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/olivernan_cohere_com/gpt_recaption_data_2024_11_01_raw/RecapMultiHiertt/train.jsonl', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        data = json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('table_data/MultiHiertt_processed.jsonl', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['',\n",
       "   'December 31, 2007',\n",
       "   'December 31, 2007',\n",
       "   'December 31, 2006',\n",
       "   'December 31, 2006'],\n",
       "  ['', 'Carrying Value', 'Fair Value', 'Carrying Value', 'Fair Value'],\n",
       "  ['', '(In thousands)', '(In thousands)', '(In thousands)', '(In thousands)'],\n",
       "  ['Mortgages and notes payable',\n",
       "   '$584,795',\n",
       "   '$603,200',\n",
       "   '$420,061',\n",
       "   '$449,130'],\n",
       "  ['Senior notes', '$977,556', '$979,562', '$1,127,508', '$1,146,767']],\n",
       " [['', 2014, 2013],\n",
       "  ['Shares outstanding at beginning of period', 1945, 1974],\n",
       "  ['Treasury stock purchases-1', -46, -27],\n",
       "  ['Other-2', 52, -2],\n",
       "  ['Shares outstanding at end of period', 1951, 1945]],\n",
       " [['',\n",
       "   'Year Ended December 31, 2010',\n",
       "   'Year Ended December 31, 2010',\n",
       "   'Year Ended December 31, 2010',\n",
       "   'Year Ended December 31, 2010',\n",
       "   'Year Ended December 31, 2010'],\n",
       "  ['',\n",
       "   'Other Assets',\n",
       "   'Separate Account Assets -4',\n",
       "   'Future Policy Benefits',\n",
       "   'Long- term Debt',\n",
       "   'Other Liabilities'],\n",
       "  ['',\n",
       "   '(in millions)',\n",
       "   '(in millions)',\n",
       "   '(in millions)',\n",
       "   '(in millions)',\n",
       "   '(in millions)'],\n",
       "  ['Fair Value, beginning of period',\n",
       "   '$27',\n",
       "   '$13,052',\n",
       "   '$-55',\n",
       "   '$-429',\n",
       "   '$-6'],\n",
       "  ['Total gains or (losses) (realized/unrealized):', '', '', '', '', ''],\n",
       "  ['Included in earnings:', '', '', '', '', ''],\n",
       "  ['Realized investment gains (losses), net', '0', '0', '570', '0', '1'],\n",
       "  ['Asset management fees and other income', '-7', '0', '0', '0', '0'],\n",
       "  ['Interest credited to policyholders’ account balances',\n",
       "   '0',\n",
       "   '2129',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0'],\n",
       "  ['Included in other comprehensive income (loss)', '0', '0', '0', '0', '0'],\n",
       "  ['Net investment income', '0', '0', '0', '0', '0'],\n",
       "  ['Purchases, sales, issuances and settlements',\n",
       "   '-11',\n",
       "   '851',\n",
       "   '-311',\n",
       "   '429',\n",
       "   '2'],\n",
       "  ['Foreign currency translation', '0', '0', '0', '0', '0'],\n",
       "  ['Other-1', '0', '0', '0', '0', '0'],\n",
       "  ['Transfers into Level 3-2', '0', '171', '0', '0', '0'],\n",
       "  ['Transfers out of Level 3-2', '0', '-411', '0', '0', '0'],\n",
       "  ['Fair Value, end of period', '$9', '$15,792', '$204', '$0', '$-3'],\n",
       "  ['Unrealized gains (losses) for the period relating to those Level 3 assets and liabilities that were still held at the end of theperiod(3):',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   ''],\n",
       "  ['Included in earnings:', '', '', '', '', ''],\n",
       "  ['Realized investment gains (losses), net', '$0', '$0', '$522', '$0', '$1'],\n",
       "  ['Asset management fees and other income', '$-7', '$0', '$0', '$0', '$0'],\n",
       "  ['Interest credited to policyholders’ account balances',\n",
       "   '$0',\n",
       "   '$1,081',\n",
       "   '$0',\n",
       "   '$0',\n",
       "   '$0'],\n",
       "  ['Included in other comprehensive income (loss)',\n",
       "   '$0',\n",
       "   '$0',\n",
       "   '$0',\n",
       "   '$0',\n",
       "   '$0']],\n",
       " [['$ in millions', 'Pension Plans', 'Medical and Life Plans'],\n",
       "  ['Year Ending December 31', '', ''],\n",
       "  ['2009', '$1,147', '$205'],\n",
       "  ['2010', '1216', '207'],\n",
       "  ['2011', '1291', '209'],\n",
       "  ['2012', '1353', '212'],\n",
       "  ['2013', '1424', '218'],\n",
       "  ['2014 through 2018', '8367', '1198']],\n",
       " [['', '2008', '2008', '2008', '2007', '2007', '2007'],\n",
       "  ['$ in millions',\n",
       "   'Funded',\n",
       "   'Unfunded',\n",
       "   'Total Backlog',\n",
       "   'Funded',\n",
       "   'Unfunded',\n",
       "   'Total Backlog'],\n",
       "  ['Information & Services', '', '', '', '', '', ''],\n",
       "  ['Mission Systems',\n",
       "   '$2,646',\n",
       "   '$3,004',\n",
       "   '$5,650',\n",
       "   '$2,365',\n",
       "   '$3,288',\n",
       "   '$5,653'],\n",
       "  ['Information Technology', '2724', '1899', '4623', '2581', '2268', '4849'],\n",
       "  ['Technical Services', '1734', '2600', '4334', '1471', '3193', '4664'],\n",
       "  ['Aerospace', '', '', '', '', '', ''],\n",
       "  ['Integrated Systems', '5759', '5122', '10881', '4204', '4525', '8729'],\n",
       "  ['Space Technology', '1889', '17761', '19650', '2295', '13963', '16258'],\n",
       "  ['Electronics', '8437', '2124', '10561', '7887', '2047', '9934'],\n",
       "  ['Shipbuilding', '14205', '8148', '22353', '10348', '3230', '13578'],\n",
       "  ['Total backlog',\n",
       "   '$37,394',\n",
       "   '$40,658',\n",
       "   '$78,052',\n",
       "   '$31,151',\n",
       "   '$32,514',\n",
       "   '$63,665']]]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data[0]['Table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['',\n",
       "  'December 31, 2007',\n",
       "  'December 31, 2007',\n",
       "  'December 31, 2006',\n",
       "  'December 31, 2006'],\n",
       " ['', 'Carrying Value', 'Fair Value', 'Carrying Value', 'Fair Value'],\n",
       " ['', '(In thousands)', '(In thousands)', '(In thousands)', '(In thousands)'],\n",
       " ['Mortgages and notes payable',\n",
       "  '$584,795',\n",
       "  '$603,200',\n",
       "  '$420,061',\n",
       "  '$449,130'],\n",
       " ['Senior notes', '$977,556', '$979,562', '$1,127,508', '$1,146,767']]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_html(data[0]['Table'][0][0])[0].fillna(\"\")\n",
    "df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>December 31, 2007</td>\n",
       "      <td>December 31, 2007</td>\n",
       "      <td>December 31, 2006</td>\n",
       "      <td>December 31, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Carrying Value</td>\n",
       "      <td>Fair Value</td>\n",
       "      <td>Carrying Value</td>\n",
       "      <td>Fair Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>(In thousands)</td>\n",
       "      <td>(In thousands)</td>\n",
       "      <td>(In thousands)</td>\n",
       "      <td>(In thousands)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mortgages and notes payable</td>\n",
       "      <td>$584,795</td>\n",
       "      <td>$603,200</td>\n",
       "      <td>$420,061</td>\n",
       "      <td>$449,130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior notes</td>\n",
       "      <td>$977,556</td>\n",
       "      <td>$979,562</td>\n",
       "      <td>$1,127,508</td>\n",
       "      <td>$1,146,767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0                  1                  2  \\\n",
       "0                               December 31, 2007  December 31, 2007   \n",
       "1                                  Carrying Value         Fair Value   \n",
       "2                                  (In thousands)     (In thousands)   \n",
       "3  Mortgages and notes payable           $584,795           $603,200   \n",
       "4                 Senior notes           $977,556           $979,562   \n",
       "\n",
       "                   3                  4  \n",
       "0  December 31, 2006  December 31, 2006  \n",
       "1     Carrying Value         Fair Value  \n",
       "2     (In thousands)     (In thousands)  \n",
       "3           $420,061           $449,130  \n",
       "4         $1,127,508         $1,146,767  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ukr_Cyrl: 7830\n",
      "arb_Arab: 7830\n",
      "ell_Grek: 7830\n",
      "por_Latn: 7830\n",
      "ind_Latn: 7830\n",
      "deu_Latn: 7830\n",
      "ces_Latn: 7830\n",
      "hin_Deva: 7830\n",
      "nld_Latn: 7830\n",
      "pes_Arab: 7830\n",
      "vie_Latn: 7830\n",
      "pol_Latn: 7830\n",
      "rus_Cyrl: 7830\n",
      "kor_Hang: 7830\n",
      "zho_Hant: 7830\n",
      "fra_Latn: 7830\n",
      "spa_Latn: 7830\n",
      "tur_Latn: 7830\n",
      "jpn_Jpan: 7830\n",
      "zho_Hans: 7830\n",
      "heb_Hebr: 7830\n",
      "ron_Latn: 7830\n",
      "ita_Latn: 7830\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for lang in os.listdir('/home/olivernan_cohere_com/recap_data_translation_2024_11_01_raw/RecapMultiHiertt_translation'):\n",
    "    with open(f'/home/olivernan_cohere_com/recap_data_translation_2024_11_01_raw/RecapMultiHiertt_translation/{lang}/train.jsonl', 'r') as f:\n",
    "        line_number = [_ for _ in f]\n",
    "        print(f\"{lang}: {len(line_number)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instructmultilingual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
